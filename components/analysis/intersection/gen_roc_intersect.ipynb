{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, '/home/jupyter/voxceleb-fairness/common/')\n",
    "sys.path.insert(0, '/home/jupyter/voxceleb-fairness/components/train/src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pwd\n",
    "import pdb\n",
    "import glob\n",
    "import time\n",
    "import yaml\n",
    "import wandb\n",
    "import numpy\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import google\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import time, os, argparse, socket\n",
    "from SpeakerNet import SpeakerNet\n",
    "from IterableTrainDataset import IterableTrainDataset\n",
    "from baseline_misc.tuneThreshold import tuneThresholdfromScore\n",
    "from utils.data_utils import download_gcs_dataset, extract_gcs_dataset, \\\n",
    "                     transcode_gcs_dataset, get_loc_paths_from_gcs_dataset,\\\n",
    "                     download_blob, upload_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: Cuda available: False\n",
      "train: Using cuda: False\n",
      "train: Torch version: 1.6.0\n",
      "train: Cuda version: 10.2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description = \"SpeakerNet\");\n",
    "\n",
    "parser.add_argument('--set-seed', action='store_true')\n",
    "parser.add_argument('--no-cuda', action='store_true', help=\"Flag to disable cuda for this run\")\n",
    "\n",
    "parser.add_argument('--checkpoint-bucket', type=str,\n",
    "        default=\"voxsrc-2020-checkpoints-dev\");\n",
    "\n",
    "# new training params\n",
    "parser.add_argument('--gaussian-noise-std', type=float, default=.9,\n",
    "        help=\"Standard deviation of gaussian noise used to augment utterance \"\n",
    "             \"spectrogram training data\");\n",
    "\n",
    "## Data loader\n",
    "parser.add_argument('--max_frames', type=int, default=200,  help='Input length to the network');\n",
    "parser.add_argument('--batch_size', type=int, default=200,  help='Batch size');\n",
    "# @TODO figure out max eval batch size for this on V100\n",
    "parser.add_argument('--eval_batch_size', type=int, default=85,  help='Batch size for loading validation data for model eval');\n",
    "# ^^^ use --batch_size=30 for small datasets that can't fill an entire 200 speaker pair/triplet batch\n",
    "parser.add_argument('--max_seg_per_spk', type=int, default=100, help='Maximum number of utterances per speaker per epoch');\n",
    "parser.add_argument('--n-data-loader-thread', type=int, default=7, help='Number of loader threads');\n",
    "\n",
    "## Training details\n",
    "# @TODO disentangle learning rate decay from validation\n",
    "parser.add_argument('--test_interval', type=int, default=10, help='Test and save every [test_interval] epochs');\n",
    "parser.add_argument('--max_epoch',      type=int, default=100, help='Maximum number of epochs');\n",
    "# ^^^ use --max_epoch=1 for local testing\n",
    "parser.add_argument('--trainfunc', type=str, default=\"angleproto\",    help='Loss function');\n",
    "parser.add_argument('--optimizer', type=str, default=\"adam\", help='sgd or adam');\n",
    "\n",
    "## Learning rates\n",
    "parser.add_argument('--lr', type=float, default=0.001,      help='Learning rate');\n",
    "parser.add_argument('--lr_decay_interval', type=int, default=10, help='Reduce the learning rate every [lr_decay_interval] epochs');\n",
    "parser.add_argument(\"--lr_decay\", type=float, default=0.95, help='Learning rate decay every [test_interval] epochs');\n",
    "\n",
    "## Loss functions\n",
    "parser.add_argument(\"--hard_prob\", type=float, default=0.5, help='Hard negative mining probability, otherwise random, only for some loss functions');\n",
    "parser.add_argument(\"--hard_rank\", type=int, default=10,    help='Hard negative mining rank in the batch, only for some loss functions');\n",
    "parser.add_argument('--margin', type=float,  default=0.3,     help='Loss margin, only for some loss functions');\n",
    "parser.add_argument('--scale', type=float,   default=30,    help='Loss scale, only for some loss functions');\n",
    "parser.add_argument('--nSpeakers', type=int, default=5994,  help='Number of speakers in the softmax layer for softmax-based losses, utterances per speaker per iteration for other losses');\n",
    "\n",
    "## Load and save\n",
    "parser.add_argument('--initial_model',  type=str, default=\"\", help='Initial model weights');\n",
    "parser.add_argument('--save_path',      type=str, default=\"/tmp/data/exp1\", help='Path for model and logs');\n",
    "\n",
    "## Training and test data\n",
    "parser.add_argument('--train_list', type=str, help='Train list');\n",
    "parser.add_argument('--test_list',  type=str, help='Evaluation list');\n",
    "parser.add_argument('--train_path', type=str, default=\"voxceleb2\", help='Absolute path to the train set');\n",
    "parser.add_argument('--test_path',  type=str, default=\"voxceleb1\", help='Absolute path to the test set');\n",
    "\n",
    "## For test only\n",
    "parser.add_argument('--eval', dest='eval', action='store_true', help='Eval only')\n",
    "\n",
    "## Model definition\n",
    "parser.add_argument('--model', type=str,        default=\"ResNetSE34L\",     help='Name of model definition');\n",
    "parser.add_argument('--encoder_type', type=str, default=\"SAP\",  help='Type of encoder');\n",
    "parser.add_argument('--nOut', type=int,         default=512,    help='Embedding size in the last FC layer');\n",
    "\n",
    "args = parser.parse_args(\"\");\n",
    "\n",
    "\n",
    "# set random seeds\n",
    "# @TODO any reason to use BOTH 'random' and 'numpy.random'?\n",
    "if args.set_seed:\n",
    "    print(\"train: Using fixed random seed\")\n",
    "    random.seed(0)\n",
    "    numpy.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "\n",
    "# set torch device to cuda or cpu\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "print(f\"train: Cuda available: {cuda_avail}\")\n",
    "use_cuda = cuda_avail and not args.no_cuda\n",
    "print(f\"train: Using cuda: {use_cuda}\")\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(f\"train: Torch version: {torch.__version__}\")\n",
    "print(f\"train: Cuda version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 512, encoder SAP.\n",
      "Initialised AngleProto\n"
     ]
    }
   ],
   "source": [
    "## Load models\n",
    "s = SpeakerNet(device, **vars(args));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from path /home/jupyter/model000000029.model\n"
     ]
    }
   ],
   "source": [
    "s.loadParameters(\"/home/jupyter/model000000029.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ROC data for 30 test lists\n",
      "SpeakerNet: Starting model eval on 6 batches of size 85\n",
      "device Type is cpu\n",
      "IterableEvalDataset: Starting worker thread #0IterableEvalDataset: Starting worker thread #2IterableEvalDataset: Starting worker thread #1\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #5\n",
      "IterableEvalDataset: Starting worker thread #4IterableEvalDataset: Starting worker thread #6IterableEvalDataset: Starting worker thread #3\n",
      "\n",
      "\n",
      "IterableEvalDataset: Stopping worker #6\n",
      "\n",
      "IterableEvalDataset: Stopping worker #3IterableEvalDataset: Stopping worker #2IterableEvalDataset: Stopping worker #1IterableEvalDataset: Stopping worker #0\n",
      "IterableEvalDataset: Stopping worker #5\n",
      "\n",
      "\n",
      "\n",
      "IterableEvalDataset: Stopping worker #4\n",
      "SpeakerNet: Computed utterance segment embeddings in 59.89487338066101 (s)\n",
      "SpeakerNet: Computed utterance test pair scores in 18.093347787857056 (s)\n",
      "Evaluated scores/labels for intersect_f_Sweden\n",
      "SpeakerNet: Starting model eval on 5 batches of size 85\n",
      "device Type is cpu\n",
      "IterableEvalDataset: Starting worker thread #1IterableEvalDataset: Starting worker thread #2\n",
      "IterableEvalDataset: Starting worker thread #0IterableEvalDataset: Starting worker thread #4\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #6\n",
      "IterableEvalDataset: Starting worker thread #5\n",
      "IterableEvalDataset: Starting worker thread #3IterableEvalDataset: Stopping worker #5\n",
      "\n",
      "\n",
      "IterableEvalDataset: Stopping worker #6IterableEvalDataset: Stopping worker #4\n",
      "\n",
      "IterableEvalDataset: Stopping worker #0IterableEvalDataset: Stopping worker #1IterableEvalDataset: Stopping worker #2\n",
      "\n",
      "IterableEvalDataset: Stopping worker #3\n",
      "\n",
      "SpeakerNet: Computed utterance segment embeddings in 41.429067611694336 (s)\n",
      "SpeakerNet: Computed utterance test pair scores in 6.426150798797607 (s)\n",
      "Evaluated scores/labels for intersect_f_Italy\n",
      "SpeakerNet: Starting model eval on 9 batches of size 85\n",
      "device Type is cpu\n",
      "IterableEvalDataset: Starting worker thread #1IterableEvalDataset: Starting worker thread #2IterableEvalDataset: Starting worker thread #0\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #4\n",
      "IterableEvalDataset: Starting worker thread #6\n",
      "IterableEvalDataset: Starting worker thread #5\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #3\n",
      "IterableEvalDataset: Stopping worker #6\n",
      "IterableEvalDataset: Stopping worker #5IterableEvalDataset: Stopping worker #3IterableEvalDataset: Stopping worker #2IterableEvalDataset: Stopping worker #4\n",
      "\n",
      "\n",
      "IterableEvalDataset: Stopping worker #0\n",
      "\n",
      "IterableEvalDataset: Stopping worker #1\n",
      "SpeakerNet: Computed utterance segment embeddings in 90.73971343040466 (s)\n",
      "SpeakerNet: Computed utterance test pair scores in 40.14905548095703 (s)\n",
      "Evaluated scores/labels for intersect_m_Germany\n",
      "SpeakerNet: Starting model eval on 34 batches of size 85\n",
      "device Type is cpu\n",
      "IterableEvalDataset: Starting worker thread #1IterableEvalDataset: Starting worker thread #0IterableEvalDataset: Starting worker thread #2\n",
      "IterableEvalDataset: Starting worker thread #4IterableEvalDataset: Starting worker thread #5\n",
      "\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #6\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #3\n",
      "IterableEvalDataset: Stopping worker #6\n",
      "IterableEvalDataset: Stopping worker #0\n",
      "IterableEvalDataset: Stopping worker #1\n",
      "IterableEvalDataset: Stopping worker #2\n",
      "IterableEvalDataset: Stopping worker #3\n",
      "IterableEvalDataset: Stopping worker #4\n",
      "IterableEvalDataset: Stopping worker #5\n",
      "SpeakerNet: Computed utterance segment embeddings in 333.577383518219 (s)\n",
      "SpeakerNet: Computed utterance test pair scores in 84.85041570663452 (s)\n",
      "Evaluated scores/labels for intersect_m_Canada\n",
      "SpeakerNet: Starting model eval on 7 batches of size 85\n",
      "device Type is cpu\n",
      "IterableEvalDataset: Starting worker thread #0IterableEvalDataset: Starting worker thread #1IterableEvalDataset: Starting worker thread #2\n",
      "\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #5\n",
      "IterableEvalDataset: Starting worker thread #4IterableEvalDataset: Starting worker thread #6\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #3\n",
      "IterableEvalDataset: Stopping worker #6\n",
      "IterableEvalDataset: Stopping worker #0\n",
      "IterableEvalDataset: Stopping worker #3\n",
      "IterableEvalDataset: Stopping worker #5IterableEvalDataset: Stopping worker #2\n",
      "IterableEvalDataset: Stopping worker #4IterableEvalDataset: Stopping worker #1\n",
      "\n",
      "\n",
      "SpeakerNet: Computed utterance segment embeddings in 69.81916046142578 (s)\n",
      "SpeakerNet: Computed utterance test pair scores in 17.629889249801636 (s)\n",
      "Evaluated scores/labels for intersect_m_Mexico\n",
      "SpeakerNet: Starting model eval on 33 batches of size 85\n",
      "device Type is cpu\n",
      "IterableEvalDataset: Starting worker thread #0IterableEvalDataset: Starting worker thread #1IterableEvalDataset: Starting worker thread #2\n",
      "IterableEvalDataset: Starting worker thread #4\n",
      "\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #6IterableEvalDataset: Starting worker thread #5\n",
      "\n",
      "IterableEvalDataset: Starting worker thread #3\n",
      "IterableEvalDataset: Stopping worker #5\n",
      "IterableEvalDataset: Stopping worker #6\n",
      "IterableEvalDataset: Stopping worker #0\n",
      "IterableEvalDataset: Stopping worker #1\n",
      "IterableEvalDataset: Stopping worker #2\n",
      "IterableEvalDataset: Stopping worker #3\n",
      "IterableEvalDataset: Stopping worker #4\n",
      "SpeakerNet: Computed utterance segment embeddings in 340.33987855911255 (s)\n",
      "SpeakerNet: Computed utterance segment embeddings in 89.15638780593872 (s)\n"
     ]
    }
   ],
   "source": [
    "spectrogram_feats_path = \"/home/jupyter/voxceleb-fairness/data/datasets/full/vox1_full_feats_milo_webster-19rvuxfu\"\n",
    "list_files_base_path = \"/home/jupyter/voxceleb-fairness/data/lists/intersect/\"\n",
    "\n",
    "# full paths to test lists\n",
    "balanced_test_list_paths = glob.glob(os.path.join(list_files_base_path, \"*balanced.txt\"))\n",
    "\n",
    "# names of each test list\n",
    "balanced_test_pair_names = [str(x).split('vox1_')[1].split('_balanced.txt')[0] for x in balanced_test_list_paths]\n",
    "\n",
    "print(f\"Generating ROC data for {len(balanced_test_list_paths)} test lists\")\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "\n",
    "fpr_tpr_and_thresholds = []\n",
    "\n",
    "# generate scores and labels by evaluating the lists against the model\n",
    "for i, test_list_path in enumerate(balanced_test_list_paths):\n",
    "    # evaluate model\n",
    "    scores, labels = s.evaluate_on(test_list_path, spectrogram_feats_path)\n",
    "    print(f\"Evaluated scores/labels for {balanced_test_pair_names[i]}\")\n",
    "    \n",
    "    # compute metrics\n",
    "    fpr_tpr_and_thresholds.append(metrics.roc_curve(labels, scores, pos_label=1))\n",
    "    \n",
    "    # plot for sanity\n",
    "    ax.scatter(fpr_tpr_and_thresholds[-1][0], fpr_tpr_and_thresholds[-1][1],\n",
    "               label=balanced_test_pair_names[i], marker='.')\n",
    "\n",
    "ax.set_aspect(1)\n",
    "fig.legend(loc='center right')\n",
    "#plt.show()\n",
    "plt.savefig('roc_intersect')\n",
    "\n",
    "# pack metrics for each test list into a dictionary\n",
    "results = {}\n",
    "for i, data in enumerate(fpr_tpr_and_thresholds):\n",
    "    name = balanced_test_pair_names[i]\n",
    "    results[name + \"_fpr\"] = data[0].tolist()\n",
    "    results[name + \"_tpr\"] = data[1].tolist()\n",
    "    results[name + \"_thresholds\"] = data[2].tolist()\n",
    "    \n",
    "# save the data as json\n",
    "dest_file_path = '/home/jupyter/voxceleb-fairness/data/roc/roc_intersect.json'\n",
    "with open(dest_file_path, 'w') as fp:\n",
    "    json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:voxsrc-2020]",
   "language": "python",
   "name": "conda-env-voxsrc-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
