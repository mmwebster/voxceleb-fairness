{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, errno\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('~/vox1_meta.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(meta_data[\"Nationality\"].value_counts())\n",
    "# print(meta_data[\"Gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @brief Get speaker IDs where their metadata value for keys[i] has value values[i]\n",
    "# @param alt_map Instead of comparing the key to value, compare map[key] to value\n",
    "def get_ids(data, keys, values):\n",
    "    for key_idx, value in enumerate(values):\n",
    "        # set data to subset with value for given key\n",
    "        data = data[data[keys[key_idx]] == value]\n",
    "        \n",
    "    return set(data['VoxCeleb1 ID'])\n",
    "\n",
    "# @brief Return list of tuples of all combinations of keys in\n",
    "#        the sensitive param provided as args\n",
    "def get_intersection_combos(*sensitive_param_value_groups):\n",
    "    return list(itertools.product(*sensitive_param_value_groups))\n",
    "\n",
    "def map_col(df, key, values_map):\n",
    "    return df.replace({key: values_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection ('m', 'english').count -> 631\n",
      "Intersection ('m', 'non-english').count -> 59\n",
      "Intersection ('f', 'english').count -> 500\n",
      "Intersection ('f', 'non-english').count -> 61\n"
     ]
    }
   ],
   "source": [
    "gender_values = ['m', 'f']\n",
    "country_values = ['Australia', 'India', 'Norway', 'Ireland', 'Germany', 'New Zealand', 'Italy','Mexico',\\\n",
    "                  'Sweden', 'Spain', 'Russia', 'Switzerland', 'Chile', 'Philippines', 'Croatia', 'Denmark',\\\n",
    "                  'Netherlands', 'Poland', 'Portugal', 'China', 'France', 'Guyana', 'Singapore', 'Brazil',\\\n",
    "                  'Sri Lanka', 'South Africa', 'South Korea', 'Trinidad and Tobago', 'Pakistan', 'Austria',\\\n",
    "                  'Israel', 'Iran', 'Sudan', 'USA', 'UK', 'Canada']\n",
    "native_lang_values = ['english', 'non-english']\n",
    "native_lang_map = {'Australia': 'english', 'India': 'non-english', 'Norway': 'non-english', 'Ireland': 'english', 'Germany': 'non-english', 'New Zealand': 'english', 'Italy': 'non-english','Mexico': 'non-english',\\\n",
    "                  'Sweden': 'non-english', 'Spain': 'non-english', 'Russia': 'non-english', 'Switzerland': 'non-english', 'Chile': 'non-english', 'Philippines': 'non-english', 'Croatia': 'non-english', 'Denmark': 'non-english',\\\n",
    "                  'Netherlands': 'non-english', 'Poland': 'non-english', 'Portugal': 'non-english', 'China': 'non-english', 'France': 'non-english', 'Guyana': 'non-english', 'Singapore': 'non-english', 'Brazil': 'non-english',\\\n",
    "                  'Sri Lanka': 'non-english', 'South Africa': 'non-english', 'South Korea': 'non-english', 'Trinidad and Tobago': 'non-english', 'Pakistan': 'non-english', 'Austria': 'non-english',\\\n",
    "                  'Israel': 'non-english', 'Iran': 'non-english', 'Sudan': 'non-english', 'USA': 'english', 'UK': 'english', 'Canada': 'english'}\n",
    "\n",
    "keys = ['Gender', 'Nationality']\n",
    "intersections_ids = []\n",
    "do_gender_nationality_intersection = False\n",
    "\n",
    "if do_gender_nationality_intersection:\n",
    "    # GENDER/NATIONALITY intersection\n",
    "    # get all combinations of sensitive param values\n",
    "    intersections_vals = get_intersection_combos(gender_values, country_values)\n",
    "    \n",
    "    # get all speaker IDs matching the combinations of sensitive param values\n",
    "    for intersection in intersections_vals:\n",
    "        intersections_ids.append(get_ids(meta_data, keys, intersection))\n",
    "        print(f\"Intersection {intersection}.count -> {len(intersections_ids[-1])}\")\n",
    "else:\n",
    "    # GENDER/NATIVE-LANG intersection\n",
    "    # replace nationality col with native-lang col\n",
    "    filtered_meta_data = map_col(meta_data, 'Nationality', native_lang_map)\n",
    "    \n",
    "    # get all combinations of sensitive param values\n",
    "    intersections_vals = get_intersection_combos(gender_values, native_lang_values)\n",
    "\n",
    "    # get all speaker IDs matching the combinations of sensitive param values\n",
    "    for intersection in intersections_vals:\n",
    "        intersections_ids.append(get_ids(filtered_meta_data, keys, intersection))\n",
    "        print(f\"Intersection {intersection}.count -> {len(intersections_ids[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all utternaces\n",
    "utters = pd.read_csv('/home/jupyter/voxceleb-fairness/data/datasets/full/vox1_full_utterances.txt', header=None, sep=' ')\n",
    "intersections_utters = []\n",
    "\n",
    "# extract all utterance subsets\n",
    "for idx, intersection_ids in enumerate(intersections_ids):\n",
    "    intersection_utters = utters[[x in intersection_ids for x in utters[0]]]\n",
    "    sample_len = min(len(intersection_utters), 5000)\n",
    "    intersections_utters.append(intersection_utters.sample(sample_len).reset_index(drop=True))\n",
    "    # print(intersections_vals[idx])\n",
    "    # print(intersections_utters[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list files from utterance subset files\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occurred\n",
    "            \n",
    "def make_pairs(utters_df, filename):\n",
    "    silentremove(filename)\n",
    "    start = time.time()\n",
    "    current_iter_start = start\n",
    "    for i in range(len(utters_df)):\n",
    "        data = []\n",
    "        for j in range(i + 1, len(utters_df)):\n",
    "            num = 1 if utters_df[0][i] == utters_df[0][j] else 0\n",
    "            data.append([num, utters_df[1][i], utters_df[1][j]])\n",
    "        pd.DataFrame(data).to_csv(filename, mode='a', index=False, header=None, sep=' ')\n",
    "        if i % 1000 == 0:\n",
    "            current_iter_end = time.time()\n",
    "            print('Wrote {} of {} utterances in {} seconds ({} seconds from start)'.format(i, len(utters_df), current_iter_end - current_iter_start, current_iter_end - start))\n",
    "            current_iter_start = current_iter_end\n",
    "\n",
    "def clean_intersection_name(name):\n",
    "    name = name.replace(')','')\n",
    "    name = name.replace('(','')\n",
    "    name = name.replace('\\'','')\n",
    "    name = name.replace(' ','')\n",
    "    name = name.replace(',','_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 of 5000 utterances in 0.30498290061950684 seconds (0.30498290061950684 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 260.7658450603485 seconds (261.070827960968 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 199.21984457969666 seconds (460.2906725406647 seconds from start)\n"
     ]
    }
   ],
   "source": [
    "# extract the full lists\n",
    "list_fnames = []\n",
    "for idx, intersection_utters in enumerate(intersections_utters):\n",
    "    # get file name for list and make dirs\n",
    "    name = clean_intersection_name(str(intersections_vals[idx]))\n",
    "    list_fnames.append(f\"/home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_{name}.txt\")\n",
    "    os.makedirs(os.path.dirname(list_fnames[-1]), exist_ok=True)\n",
    "    # write the list\n",
    "    make_pairs(intersection_utters, list_fnames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the balanced lists with equal positive and negative test pairs\n",
    "def balance_pairs(unbalanced_pairs):\n",
    "    return pd.concat([unbalanced_pairs[unbalanced_pairs[0] == 1].reset_index(drop=True),\\\n",
    "               unbalanced_pairs[unbalanced_pairs[0] == 0].sample(sum(unbalanced_pairs[0] == 1)).\\\n",
    "               reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "\n",
    "for list_fname in list_fnames:\n",
    "    try:\n",
    "        print(f\"Running {list_fname}\")\n",
    "        # set new file name\n",
    "        new_fname = list_fname.replace('.txt', '_balanced.txt')\n",
    "        # read in old data and etract balanced list\n",
    "        pairs = pd.read_csv(list_fname, header=None, sep=' ')\n",
    "        balanced_pairs = balance_pairs(pairs)\n",
    "        # write the new list\n",
    "        balanced_pairs.to_csv(new_fname, index=False, header=None, sep=' ')\n",
    "    except ValueError:\n",
    "        print(f\">> File is empty... skipping\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\">> File not found... skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:voxsrc-2020]",
   "language": "python",
   "name": "conda-env-voxsrc-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
