{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, errno\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('~/vox1_meta.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(meta_data[\"Nationality\"].value_counts())\n",
    "# print(meta_data[\"Gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @brief Get speaker IDs where their metadata value for keys[i] has value values[i]\n",
    "# @param alt_map Instead of comparing the key to value, compare map[key] to value\n",
    "def get_ids(data, keys, values):\n",
    "    for key_idx, value in enumerate(values):\n",
    "        # set data to subset with value for given key\n",
    "        data = data[data[keys[key_idx]] == value]\n",
    "        \n",
    "    return set(data['VoxCeleb1 ID'])\n",
    "\n",
    "# @brief Return list of tuples of all combinations of keys in\n",
    "#        the sensitive param provided as args\n",
    "def get_intersection_combos(*sensitive_param_value_groups):\n",
    "    return list(itertools.product(*sensitive_param_value_groups))\n",
    "\n",
    "def map_col(df, key, values_map):\n",
    "    return df.replace({key: values_map})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection ('m', 'Australia').count -> 25\n",
      "Intersection ('m', 'India').count -> 15\n",
      "Intersection ('m', 'Norway').count -> 13\n",
      "Intersection ('m', 'Ireland').count -> 13\n",
      "Intersection ('m', 'Germany').count -> 4\n",
      "Intersection ('m', 'New Zealand').count -> 6\n",
      "Intersection ('m', 'Italy').count -> 3\n",
      "Intersection ('m', 'Mexico').count -> 5\n",
      "Intersection ('m', 'Sweden').count -> 1\n",
      "Intersection ('m', 'Spain').count -> 2\n",
      "Intersection ('m', 'Russia').count -> 0\n",
      "Intersection ('m', 'Switzerland').count -> 3\n",
      "Intersection ('m', 'Chile').count -> 2\n",
      "Intersection ('m', 'Philippines').count -> 1\n",
      "Intersection ('m', 'Croatia').count -> 2\n",
      "Intersection ('m', 'Denmark').count -> 1\n",
      "Intersection ('m', 'Netherlands').count -> 1\n",
      "Intersection ('m', 'Poland').count -> 0\n",
      "Intersection ('m', 'Portugal').count -> 1\n",
      "Intersection ('m', 'China').count -> 1\n",
      "Intersection ('m', 'France').count -> 0\n",
      "Intersection ('m', 'Guyana').count -> 0\n",
      "Intersection ('m', 'Singapore').count -> 1\n",
      "Intersection ('m', 'Brazil').count -> 0\n",
      "Intersection ('m', 'Sri Lanka').count -> 0\n",
      "Intersection ('m', 'South Africa').count -> 1\n",
      "Intersection ('m', 'South Korea').count -> 0\n",
      "Intersection ('m', 'Trinidad and Tobago').count -> 0\n",
      "Intersection ('m', 'Pakistan').count -> 1\n",
      "Intersection ('m', 'Austria').count -> 0\n",
      "Intersection ('m', 'Israel').count -> 0\n",
      "Intersection ('m', 'Iran').count -> 0\n",
      "Intersection ('m', 'Sudan').count -> 1\n",
      "Intersection ('m', 'USA').count -> 431\n",
      "Intersection ('m', 'UK').count -> 127\n",
      "Intersection ('m', 'Canada').count -> 29\n",
      "Intersection ('f', 'Australia').count -> 12\n",
      "Intersection ('f', 'India').count -> 11\n",
      "Intersection ('f', 'Norway').count -> 7\n",
      "Intersection ('f', 'Ireland').count -> 5\n",
      "Intersection ('f', 'Germany').count -> 5\n",
      "Intersection ('f', 'New Zealand').count -> 2\n",
      "Intersection ('f', 'Italy').count -> 5\n",
      "Intersection ('f', 'Mexico').count -> 2\n",
      "Intersection ('f', 'Sweden').count -> 4\n",
      "Intersection ('f', 'Spain').count -> 2\n",
      "Intersection ('f', 'Russia').count -> 4\n",
      "Intersection ('f', 'Switzerland').count -> 0\n",
      "Intersection ('f', 'Chile').count -> 1\n",
      "Intersection ('f', 'Philippines').count -> 2\n",
      "Intersection ('f', 'Croatia').count -> 1\n",
      "Intersection ('f', 'Denmark').count -> 2\n",
      "Intersection ('f', 'Netherlands').count -> 2\n",
      "Intersection ('f', 'Poland').count -> 2\n",
      "Intersection ('f', 'Portugal').count -> 1\n",
      "Intersection ('f', 'China').count -> 1\n",
      "Intersection ('f', 'France').count -> 1\n",
      "Intersection ('f', 'Guyana').count -> 1\n",
      "Intersection ('f', 'Singapore').count -> 0\n",
      "Intersection ('f', 'Brazil').count -> 1\n",
      "Intersection ('f', 'Sri Lanka').count -> 1\n",
      "Intersection ('f', 'South Africa').count -> 0\n",
      "Intersection ('f', 'South Korea').count -> 1\n",
      "Intersection ('f', 'Trinidad and Tobago').count -> 1\n",
      "Intersection ('f', 'Pakistan').count -> 0\n",
      "Intersection ('f', 'Austria').count -> 1\n",
      "Intersection ('f', 'Israel').count -> 1\n",
      "Intersection ('f', 'Iran').count -> 1\n",
      "Intersection ('f', 'Sudan').count -> 0\n",
      "Intersection ('f', 'USA').count -> 368\n",
      "Intersection ('f', 'UK').count -> 88\n",
      "Intersection ('f', 'Canada').count -> 25\n"
     ]
    }
   ],
   "source": [
    "gender_values = ['m', 'f']\n",
    "country_values = ['Australia', 'India', 'Norway', 'Ireland', 'Germany', 'New Zealand', 'Italy','Mexico',\\\n",
    "                  'Sweden', 'Spain', 'Russia', 'Switzerland', 'Chile', 'Philippines', 'Croatia', 'Denmark',\\\n",
    "                  'Netherlands', 'Poland', 'Portugal', 'China', 'France', 'Guyana', 'Singapore', 'Brazil',\\\n",
    "                  'Sri Lanka', 'South Africa', 'South Korea', 'Trinidad and Tobago', 'Pakistan', 'Austria',\\\n",
    "                  'Israel', 'Iran', 'Sudan', 'USA', 'UK', 'Canada']\n",
    "\n",
    "native_lang_map = {'Australia': 'english', 'India': 'non-english', 'Norway': 'non-english', 'Ireland': 'english', 'Germany': 'non-english', 'New Zealand': 'english', 'Italy': 'non-english','Mexico': 'non-english',\\\n",
    "                  'Sweden': 'non-english', 'Spain': 'non-english', 'Russia': 'non-english', 'Switzerland': 'non-english', 'Chile': 'non-english', 'Philippines': 'non-english', 'Croatia': 'non-english', 'Denmark': 'non-english',\\\n",
    "                  'Netherlands': 'non-english', 'Poland': 'non-english', 'Portugal': 'non-english', 'China': 'non-english', 'France': 'non-english', 'Guyana': 'non-english', 'Singapore': 'non-english', 'Brazil': 'non-english',\\\n",
    "                  'Sri Lanka': 'non-english', 'South Africa': 'non-english', 'South Korea': 'non-english', 'Trinidad and Tobago': 'non-english', 'Pakistan': 'non-english', 'Austria': 'non-english',\\\n",
    "                  'Israel': 'non-english', 'Iran': 'non-english', 'Sudan': 'non-english', 'USA': 'english', 'UK': 'english', 'Canada': 'english'}\n",
    "native_lang_values = list(set(native_lang_map.values()))\n",
    "\n",
    "native_lang_map_full = {'Australia': 'English', 'India': 'Hindi', 'Norway': 'Norwegian', 'Ireland': 'English', 'Germany': 'German', 'New Zealand': 'English', 'Italy': 'Italian','Mexico': 'Spanish',\\\n",
    "                  'Sweden': 'Swedish', 'Spain': 'Spanish', 'Russia': 'Russian', 'Switzerland': 'Mixed', 'Chile': 'Spanish', 'Philippines': 'English', 'Croatia': 'Croatian', 'Denmark': 'Danish',\\\n",
    "                  'Netherlands': 'Dutch', 'Poland': 'Polish', 'Portugal': 'Portuguese', 'China': 'Mandarin', 'France': 'French', 'Guyana': 'English', 'Singapore': 'English', 'Brazil': 'Portuguese',\\\n",
    "                  'Sri Lanka': 'Sinhala', 'South Africa': 'Afrikaans', 'South Korea': 'Korean', 'Trinidad and Tobago': 'English', 'Pakistan': 'English', 'Austria': 'German',\\\n",
    "                  'Israel': 'Hebrew', 'Iran': 'Persian', 'Sudan': 'Arabic', 'USA': 'English', 'UK': 'English', 'Canada': 'English'}\n",
    "native_lang_values_full = list(set(native_lang_map_full.values()))\n",
    "\n",
    "keys = ['Gender', 'Nationality']\n",
    "intersections_ids = []\n",
    "do_intersection = \"gender-nationality\"\n",
    "#do_intersection = \"gender-native-lang\"\n",
    "#do_intersection = \"gender-native-lang-full\"\n",
    "\n",
    "if do_intersection == \"gender-nationality\":\n",
    "    # GENDER/NATIONALITY intersection\n",
    "    # get all combinations of sensitive param values\n",
    "    intersections_vals = get_intersection_combos(gender_values, country_values)\n",
    "    \n",
    "    # get all speaker IDs matching the combinations of sensitive param values\n",
    "    for intersection in intersections_vals:\n",
    "        intersections_ids.append(get_ids(meta_data, keys, intersection))\n",
    "        print(f\"Intersection {intersection}.count -> {len(intersections_ids[-1])}\")\n",
    "elif do_intersection == \"gender-native-lang\":\n",
    "    # GENDER/NATIVE-LANG intersection\n",
    "    # replace nationality col with native-lang col\n",
    "    filtered_meta_data = map_col(meta_data, 'Nationality', native_lang_map)\n",
    "    \n",
    "    # get all combinations of sensitive param values\n",
    "    intersections_vals = get_intersection_combos(gender_values, native_lang_values)\n",
    "\n",
    "    # get all speaker IDs matching the combinations of sensitive param values\n",
    "    for intersection in intersections_vals:\n",
    "        intersections_ids.append(get_ids(filtered_meta_data, keys, intersection))\n",
    "        print(f\"Intersection {intersection}.count -> {len(intersections_ids[-1])}\")\n",
    "elif do_intersection == \"gender-native-lang-full\":\n",
    "    # GENDER/NATIVE-LANG intersection\n",
    "    # replace nationality col with native-lang col\n",
    "    filtered_meta_data = map_col(meta_data, 'Nationality', native_lang_map_full)\n",
    "    \n",
    "    # get all combinations of sensitive param values\n",
    "    intersections_vals = get_intersection_combos(gender_values, native_lang_values_full)\n",
    "\n",
    "    # get all speaker IDs matching the combinations of sensitive param values\n",
    "    for intersection in intersections_vals:\n",
    "        ids = get_ids(filtered_meta_data, keys, intersection)\n",
    "        status = \"\"\n",
    "        if len(ids) >= 2:\n",
    "            intersections_ids.append(ids)\n",
    "        else:\n",
    "            intersections_ids.append(None)\n",
    "            status = \"-- [skipped] \"\n",
    "            \n",
    "        print(f\"{status}Intersection {intersection}.count -> {len(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all utternaces\n",
    "utters = pd.read_csv('/home/jupyter/voxceleb-fairness/data/datasets/full/vox1_full_utterances.txt', header=None, sep=' ')\n",
    "intersections_utters = []\n",
    "\n",
    "# extract all utterance subsets\n",
    "for idx, intersection_ids in enumerate(intersections_ids):\n",
    "    if intersection_ids:\n",
    "        intersection_utters = utters[[x in intersection_ids for x in utters[0]]]\n",
    "        sample_len = min(len(intersection_utters), 5000)\n",
    "        intersections_utters.append(intersection_utters.sample(sample_len).reset_index(drop=True))\n",
    "    else:\n",
    "        intersections_utters.append(None)\n",
    "    # print(intersections_vals[idx])\n",
    "    # print(intersections_utters[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list files from utterance subset files\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occurred\n",
    "            \n",
    "def make_pairs(utters_df, filename):\n",
    "    silentremove(filename)\n",
    "    start = time.time()\n",
    "    current_iter_start = start\n",
    "    for i in range(len(utters_df)):\n",
    "        data = []\n",
    "        for j in range(i + 1, len(utters_df)):\n",
    "            num = 1 if utters_df[0][i] == utters_df[0][j] else 0\n",
    "            data.append([num, utters_df[1][i], utters_df[1][j]])\n",
    "        pd.DataFrame(data).to_csv(filename, mode='a', index=False, header=None, sep=' ')\n",
    "        if i % 1000 == 0:\n",
    "            current_iter_end = time.time()\n",
    "            print('Wrote {} of {} utterances in {} seconds ({} seconds from start)'.format(i, len(utters_df), current_iter_end - current_iter_start, current_iter_end - start))\n",
    "            current_iter_start = current_iter_end\n",
    "\n",
    "def clean_intersection_name(name):\n",
    "    name = name.replace(')','')\n",
    "    name = name.replace('(','')\n",
    "    name = name.replace('\\'','')\n",
    "    name = name.replace(' ','')\n",
    "    name = name.replace(',','_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 of 197 utterances in 0.021676063537597656 seconds (0.021676063537597656 seconds from start)\n",
      "Wrote 0 of 202 utterances in 0.01659870147705078 seconds (0.01659870147705078 seconds from start)\n",
      "Wrote 0 of 742 utterances in 0.043233633041381836 seconds (0.043233633041381836 seconds from start)\n",
      "Wrote 0 of 1744 utterances in 0.09528255462646484 seconds (0.09528255462646484 seconds from start)\n",
      "Wrote 1000 of 1744 utterances in 70.82899522781372 seconds (70.92427778244019 seconds from start)\n",
      "Wrote 0 of 5000 utterances in 0.27794718742370605 seconds (0.27794718742370605 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 254.6999740600586 seconds (254.9779212474823 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 199.0095100402832 seconds (453.9874312877655 seconds from start)\n",
      "Wrote 3000 of 5000 utterances in 141.56716990470886 seconds (595.5546011924744 seconds from start)\n",
      "Wrote 4000 of 5000 utterances in 85.47222328186035 seconds (681.0268244743347 seconds from start)\n",
      "Wrote 0 of 3034 utterances in 0.1635301113128662 seconds (0.1635301113128662 seconds from start)\n",
      "Wrote 1000 of 3034 utterances in 143.89638662338257 seconds (144.05991673469543 seconds from start)\n",
      "Wrote 2000 of 3034 utterances in 86.54054832458496 seconds (230.6004650592804 seconds from start)\n",
      "Wrote 3000 of 3034 utterances in 30.966336250305176 seconds (261.56680130958557 seconds from start)\n",
      "Wrote 0 of 807 utterances in 0.04778623580932617 seconds (0.04778623580932617 seconds from start)\n",
      "Wrote 0 of 399 utterances in 0.0232698917388916 seconds (0.0232698917388916 seconds from start)\n",
      "Wrote 0 of 186 utterances in 0.011775493621826172 seconds (0.011775493621826172 seconds from start)\n",
      "Wrote 0 of 346 utterances in 0.022557497024536133 seconds (0.022557497024536133 seconds from start)\n",
      "Wrote 0 of 920 utterances in 0.05091261863708496 seconds (0.05091261863708496 seconds from start)\n",
      "Wrote 0 of 764 utterances in 0.04506349563598633 seconds (0.04506349563598633 seconds from start)\n",
      "Wrote 0 of 5000 utterances in 0.2854034900665283 seconds (0.2854034900665283 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 252.5891535282135 seconds (252.87455701828003 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 197.56737971305847 seconds (450.4419367313385 seconds from start)\n",
      "Wrote 3000 of 5000 utterances in 140.5672516822815 seconds (591.00918841362 seconds from start)\n",
      "Wrote 4000 of 5000 utterances in 85.65663552284241 seconds (676.6658239364624 seconds from start)\n",
      "Wrote 0 of 497 utterances in 0.029253244400024414 seconds (0.029253244400024414 seconds from start)\n",
      "Wrote 0 of 657 utterances in 0.03974199295043945 seconds (0.03974199295043945 seconds from start)\n",
      "Wrote 0 of 2435 utterances in 0.14042901992797852 seconds (0.14042901992797852 seconds from start)\n",
      "Wrote 1000 of 2435 utterances in 109.50995111465454 seconds (109.65038013458252 seconds from start)\n",
      "Wrote 2000 of 2435 utterances in 52.968191623687744 seconds (162.61857175827026 seconds from start)\n",
      "Wrote 0 of 155 utterances in 0.010740518569946289 seconds (0.010740518569946289 seconds from start)\n",
      "Wrote 0 of 540 utterances in 0.030701875686645508 seconds (0.030701875686645508 seconds from start)\n",
      "Wrote 0 of 125 utterances in 0.008418798446655273 seconds (0.008418798446655273 seconds from start)\n",
      "Wrote 0 of 216 utterances in 0.01327824592590332 seconds (0.01327824592590332 seconds from start)\n"
     ]
    }
   ],
   "source": [
    "# extract the full lists\n",
    "list_fnames = []\n",
    "for idx, intersection_utters in enumerate(intersections_utters):\n",
    "    if intersection_utters is not None:\n",
    "        # get file name for list and make dirs\n",
    "        name = clean_intersection_name(str(intersections_vals[idx]))\n",
    "        list_fnames.append(f\"/home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_{name}.txt\")\n",
    "        os.makedirs(os.path.dirname(list_fnames[-1]), exist_ok=True)\n",
    "        # write the list\n",
    "        make_pairs(intersection_utters, list_fnames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Croatian.txt\n",
      ">> File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Italian.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_German.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Norwegian.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_English.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Hindi.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Spanish.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Mixed.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Danish.txt\n",
      ">> File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Italian.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_German.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Norwegian.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_English.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Swedish.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Russian.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Hindi.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Dutch.txt\n",
      ">> File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Spanish.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Portuguese.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Polish.txt\n",
      ">> File is empty... skipping\n"
     ]
    }
   ],
   "source": [
    "# create the balanced lists with equal positive and negative test pairs\n",
    "def balance_pairs(unbalanced_pairs):\n",
    "    return pd.concat([unbalanced_pairs[unbalanced_pairs[0] == 1].reset_index(drop=True),\\\n",
    "               unbalanced_pairs[unbalanced_pairs[0] == 0].sample(sum(unbalanced_pairs[0] == 1)).\\\n",
    "               reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "\n",
    "for list_fname in list_fnames:\n",
    "    try:\n",
    "        print(f\"Running {list_fname}\")\n",
    "        # set new file name\n",
    "        new_fname = list_fname.replace('.txt', '_balanced.txt')\n",
    "        # read in old data and etract balanced list\n",
    "        pairs = pd.read_csv(list_fname, header=None, sep=' ')\n",
    "        balanced_pairs = balance_pairs(pairs)\n",
    "        # write the new list\n",
    "        balanced_pairs.to_csv(new_fname, index=False, header=None, sep=' ')\n",
    "    except ValueError:\n",
    "        print(f\">> File is empty... skipping\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\">> File not found... skipping\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:voxsrc-2020]",
   "language": "python",
   "name": "conda-env-voxsrc-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
