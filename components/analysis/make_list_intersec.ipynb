{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, errno\n",
    "import time\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv('~/vox1_meta.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(meta_data[\"Nationality\"].value_counts())\n",
    "# print(meta_data[\"Gender\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(data, keys, values):\n",
    "    for key_idx, value in enumerate(values):\n",
    "        # set data to subset with value for given key\n",
    "        data = data[data[keys[key_idx]] == value]\n",
    "        \n",
    "    return set(data['VoxCeleb1 ID'])\n",
    "\n",
    "# @brief Return list of tuples of all combinations of keys in\n",
    "#        the sensitive param provided as args\n",
    "def get_intersection_combos(*sensitive_param_value_groups):\n",
    "    return list(itertools.product(*sensitive_param_value_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection ('m', 'Australia').count -> 25\n",
      "Intersection ('m', 'India').count -> 15\n",
      "Intersection ('m', 'Norway').count -> 13\n",
      "Intersection ('m', 'Ireland').count -> 13\n",
      "Intersection ('m', 'Germany').count -> 4\n",
      "Intersection ('m', 'New Zealand').count -> 6\n",
      "Intersection ('m', 'Italy').count -> 3\n",
      "Intersection ('m', 'Mexico').count -> 5\n",
      "Intersection ('m', 'Sweden').count -> 1\n",
      "Intersection ('m', 'Spain').count -> 2\n",
      "Intersection ('m', 'Russia').count -> 0\n",
      "Intersection ('m', 'Switzerland').count -> 3\n",
      "Intersection ('m', 'Chile').count -> 2\n",
      "Intersection ('m', 'Philippines').count -> 1\n",
      "Intersection ('m', 'Croatia').count -> 2\n",
      "Intersection ('m', 'Denmark').count -> 1\n",
      "Intersection ('m', 'Netherlands').count -> 1\n",
      "Intersection ('m', 'Poland').count -> 0\n",
      "Intersection ('m', 'Portugal').count -> 1\n",
      "Intersection ('m', 'China').count -> 1\n",
      "Intersection ('m', 'France').count -> 0\n",
      "Intersection ('m', 'Guyana').count -> 0\n",
      "Intersection ('m', 'Singapore').count -> 1\n",
      "Intersection ('m', 'Brazil').count -> 0\n",
      "Intersection ('m', 'Sri Lanka').count -> 0\n",
      "Intersection ('m', 'South Africa').count -> 1\n",
      "Intersection ('m', 'South Korea').count -> 0\n",
      "Intersection ('m', 'Trinidad and Tobago').count -> 0\n",
      "Intersection ('m', 'Pakistan').count -> 1\n",
      "Intersection ('m', 'Austria').count -> 0\n",
      "Intersection ('m', 'Israel').count -> 0\n",
      "Intersection ('m', 'Iran').count -> 0\n",
      "Intersection ('m', 'Sudan').count -> 1\n",
      "Intersection ('f', 'Australia').count -> 12\n",
      "Intersection ('f', 'India').count -> 11\n",
      "Intersection ('f', 'Norway').count -> 7\n",
      "Intersection ('f', 'Ireland').count -> 5\n",
      "Intersection ('f', 'Germany').count -> 5\n",
      "Intersection ('f', 'New Zealand').count -> 2\n",
      "Intersection ('f', 'Italy').count -> 5\n",
      "Intersection ('f', 'Mexico').count -> 2\n",
      "Intersection ('f', 'Sweden').count -> 4\n",
      "Intersection ('f', 'Spain').count -> 2\n",
      "Intersection ('f', 'Russia').count -> 4\n",
      "Intersection ('f', 'Switzerland').count -> 0\n",
      "Intersection ('f', 'Chile').count -> 1\n",
      "Intersection ('f', 'Philippines').count -> 2\n",
      "Intersection ('f', 'Croatia').count -> 1\n",
      "Intersection ('f', 'Denmark').count -> 2\n",
      "Intersection ('f', 'Netherlands').count -> 2\n",
      "Intersection ('f', 'Poland').count -> 2\n",
      "Intersection ('f', 'Portugal').count -> 1\n",
      "Intersection ('f', 'China').count -> 1\n",
      "Intersection ('f', 'France').count -> 1\n",
      "Intersection ('f', 'Guyana').count -> 1\n",
      "Intersection ('f', 'Singapore').count -> 0\n",
      "Intersection ('f', 'Brazil').count -> 1\n",
      "Intersection ('f', 'Sri Lanka').count -> 1\n",
      "Intersection ('f', 'South Africa').count -> 0\n",
      "Intersection ('f', 'South Korea').count -> 1\n",
      "Intersection ('f', 'Trinidad and Tobago').count -> 1\n",
      "Intersection ('f', 'Pakistan').count -> 0\n",
      "Intersection ('f', 'Austria').count -> 1\n",
      "Intersection ('f', 'Israel').count -> 1\n",
      "Intersection ('f', 'Iran').count -> 1\n",
      "Intersection ('f', 'Sudan').count -> 0\n"
     ]
    }
   ],
   "source": [
    "keys = ['Gender', 'Nationality']\n",
    "gender_values = ['m', 'f']\n",
    "# @TODO Add all countries\n",
    "#country_values = ['USA', 'UK', 'Canada']\n",
    "country_values = ['Australia', 'India', 'Norway', 'Ireland', 'Germany', 'New Zealand', 'Italy','Mexico',\\\n",
    "                  'Sweden', 'Spain', 'Russia', 'Switzerland', 'Chile', 'Philippines', 'Croatia', 'Denmark',\\\n",
    "                  'Netherlands', 'Poland', 'Portugal', 'China', 'France', 'Guyana', 'Singapore', 'Brazil',\\\n",
    "                  'Sri Lanka', 'South Africa', 'South Korea', 'Trinidad and Tobago', 'Pakistan', 'Austria',\\\n",
    "                  'Israel', 'Iran', 'Sudan']\n",
    "\n",
    "intersections_vals = get_intersection_combos(gender_values, country_values)\n",
    "intersections_ids = []\n",
    "\n",
    "for intersection in intersections_vals:\n",
    "    intersections_ids.append(get_ids(meta_data, keys, intersection))\n",
    "    print(f\"Intersection {intersection}.count -> {len(intersections_ids[-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all utternaces\n",
    "utters = pd.read_csv('/home/jupyter/voxceleb-fairness/data/datasets/full/vox1_full_utterances.txt', header=None, sep=' ')\n",
    "intersections_utters = []\n",
    "\n",
    "# extract all utterance subsets\n",
    "for idx, intersection_ids in enumerate(intersections_ids):\n",
    "    intersection_utters = utters[[x in intersection_ids for x in utters[0]]]\n",
    "    sample_len = min(len(intersection_utters), 5000)\n",
    "    intersections_utters.append(intersection_utters.sample(sample_len).reset_index(drop=True))\n",
    "    # print(intersections_vals[idx])\n",
    "    # print(intersections_utters[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list files from utterance subset files\n",
    "\n",
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occurred\n",
    "            \n",
    "def make_pairs(utters_df, filename):\n",
    "    silentremove(filename)\n",
    "    start = time.time()\n",
    "    current_iter_start = start\n",
    "    for i in range(len(utters_df)):\n",
    "        data = []\n",
    "        for j in range(i + 1, len(utters_df)):\n",
    "            num = 1 if utters_df[0][i] == utters_df[0][j] else 0\n",
    "            data.append([num, utters_df[1][i], utters_df[1][j]])\n",
    "        pd.DataFrame(data).to_csv(filename, mode='a', index=False, header=None, sep=' ')\n",
    "        if i % 1000 == 0:\n",
    "            current_iter_end = time.time()\n",
    "            print('Wrote {} of {} utterances in {} seconds ({} seconds from start)'.format(i, len(utters_df), current_iter_end - current_iter_start, current_iter_end - start))\n",
    "            current_iter_start = current_iter_end\n",
    "\n",
    "def clean_intersection_name(name):\n",
    "    name = name.replace(')','')\n",
    "    name = name.replace('(','')\n",
    "    name = name.replace('\\'','')\n",
    "    name = name.replace(' ','')\n",
    "    name = name.replace(',','_')\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 of 3157 utterances in 0.18897175788879395 seconds (0.18897175788879395 seconds from start)\n",
      "Wrote 1000 of 3157 utterances in 156.11747312545776 seconds (156.30644488334656 seconds from start)\n",
      "Wrote 2000 of 3157 utterances in 96.4682207107544 seconds (252.77466559410095 seconds from start)\n",
      "Wrote 3000 of 3157 utterances in 39.568827867507935 seconds (292.3434934616089 seconds from start)\n",
      "Wrote 0 of 3034 utterances in 0.17218542098999023 seconds (0.17218542098999023 seconds from start)\n",
      "Wrote 1000 of 3034 utterances in 148.18790888786316 seconds (148.36009430885315 seconds from start)\n",
      "Wrote 2000 of 3034 utterances in 89.34569311141968 seconds (237.70578742027283 seconds from start)\n",
      "Wrote 3000 of 3034 utterances in 32.033486127853394 seconds (269.7392735481262 seconds from start)\n",
      "Wrote 0 of 1744 utterances in 0.10055708885192871 seconds (0.10055708885192871 seconds from start)\n",
      "Wrote 1000 of 1744 utterances in 73.05301356315613 seconds (73.15357065200806 seconds from start)\n",
      "Wrote 0 of 2031 utterances in 0.11691975593566895 seconds (0.11691975593566895 seconds from start)\n",
      "Wrote 1000 of 2031 utterances in 88.76211500167847 seconds (88.87903475761414 seconds from start)\n",
      "Wrote 2000 of 2031 utterances in 32.35963249206543 seconds (121.23866724967957 seconds from start)\n",
      "Wrote 0 of 742 utterances in 0.043060302734375 seconds (0.043060302734375 seconds from start)\n",
      "Wrote 0 of 1133 utterances in 0.06602001190185547 seconds (0.06602001190185547 seconds from start)\n",
      "Wrote 1000 of 1133 utterances in 38.01499962806702 seconds (38.08101963996887 seconds from start)\n",
      "Wrote 0 of 202 utterances in 0.013109683990478516 seconds (0.013109683990478516 seconds from start)\n",
      "Wrote 0 of 572 utterances in 0.03385448455810547 seconds (0.03385448455810547 seconds from start)\n",
      "Wrote 0 of 88 utterances in 0.0063931941986083984 seconds (0.0063931941986083984 seconds from start)\n",
      "Wrote 0 of 124 utterances in 0.008491754531860352 seconds (0.008491754531860352 seconds from start)\n",
      "Wrote 0 of 399 utterances in 0.024281024932861328 seconds (0.024281024932861328 seconds from start)\n",
      "Wrote 0 of 111 utterances in 0.007849693298339844 seconds (0.007849693298339844 seconds from start)\n",
      "Wrote 0 of 141 utterances in 0.009914398193359375 seconds (0.009914398193359375 seconds from start)\n",
      "Wrote 0 of 197 utterances in 0.012621641159057617 seconds (0.012621641159057617 seconds from start)\n",
      "Wrote 0 of 97 utterances in 0.007607698440551758 seconds (0.007607698440551758 seconds from start)\n",
      "Wrote 0 of 163 utterances in 0.01172780990600586 seconds (0.01172780990600586 seconds from start)\n",
      "Wrote 0 of 178 utterances in 0.011753559112548828 seconds (0.011753559112548828 seconds from start)\n",
      "Wrote 0 of 265 utterances in 0.016463756561279297 seconds (0.016463756561279297 seconds from start)\n",
      "Wrote 0 of 62 utterances in 0.00485682487487793 seconds (0.00485682487487793 seconds from start)\n",
      "Wrote 0 of 261 utterances in 0.019510507583618164 seconds (0.019510507583618164 seconds from start)\n",
      "Wrote 0 of 94 utterances in 0.0068149566650390625 seconds (0.0068149566650390625 seconds from start)\n",
      "Wrote 0 of 196 utterances in 0.012528419494628906 seconds (0.012528419494628906 seconds from start)\n",
      "Wrote 0 of 1436 utterances in 0.08297061920166016 seconds (0.08297061920166016 seconds from start)\n",
      "Wrote 1000 of 1436 utterances in 55.09932065010071 seconds (55.18229126930237 seconds from start)\n",
      "Wrote 0 of 2435 utterances in 0.13902568817138672 seconds (0.13902568817138672 seconds from start)\n",
      "Wrote 1000 of 2435 utterances in 112.63845801353455 seconds (112.77748370170593 seconds from start)\n",
      "Wrote 2000 of 2435 utterances in 54.8489625453949 seconds (167.62644624710083 seconds from start)\n",
      "Wrote 0 of 764 utterances in 0.046112060546875 seconds (0.046112060546875 seconds from start)\n",
      "Wrote 0 of 536 utterances in 0.03144669532775879 seconds (0.03144669532775879 seconds from start)\n",
      "Wrote 0 of 642 utterances in 0.03863978385925293 seconds (0.03863978385925293 seconds from start)\n",
      "Wrote 0 of 174 utterances in 0.011560916900634766 seconds (0.011560916900634766 seconds from start)\n",
      "Wrote 0 of 346 utterances in 0.021132230758666992 seconds (0.021132230758666992 seconds from start)\n",
      "Wrote 0 of 349 utterances in 0.021416425704956055 seconds (0.021416425704956055 seconds from start)\n",
      "Wrote 0 of 497 utterances in 0.029421329498291016 seconds (0.029421329498291016 seconds from start)\n",
      "Wrote 0 of 120 utterances in 0.008065462112426758 seconds (0.008065462112426758 seconds from start)\n",
      "Wrote 0 of 657 utterances in 0.03842759132385254 seconds (0.03842759132385254 seconds from start)\n",
      "Wrote 0 of 71 utterances in 0.005308389663696289 seconds (0.005308389663696289 seconds from start)\n",
      "Wrote 0 of 346 utterances in 0.02150440216064453 seconds (0.02150440216064453 seconds from start)\n",
      "Wrote 0 of 92 utterances in 0.0065457820892333984 seconds (0.0065457820892333984 seconds from start)\n",
      "Wrote 0 of 186 utterances in 0.011991024017333984 seconds (0.011991024017333984 seconds from start)\n",
      "Wrote 0 of 155 utterances in 0.010390520095825195 seconds (0.010390520095825195 seconds from start)\n",
      "Wrote 0 of 216 utterances in 0.013726234436035156 seconds (0.013726234436035156 seconds from start)\n",
      "Wrote 0 of 59 utterances in 0.004706144332885742 seconds (0.004706144332885742 seconds from start)\n",
      "Wrote 0 of 111 utterances in 0.007815361022949219 seconds (0.007815361022949219 seconds from start)\n",
      "Wrote 0 of 87 utterances in 0.006318330764770508 seconds (0.006318330764770508 seconds from start)\n",
      "Wrote 0 of 57 utterances in 0.005110263824462891 seconds (0.005110263824462891 seconds from start)\n",
      "Wrote 0 of 66 utterances in 0.005127429962158203 seconds (0.005127429962158203 seconds from start)\n",
      "Wrote 0 of 162 utterances in 0.010652780532836914 seconds (0.010652780532836914 seconds from start)\n",
      "Wrote 0 of 53 utterances in 0.004350900650024414 seconds (0.004350900650024414 seconds from start)\n",
      "Wrote 0 of 58 utterances in 0.004610538482666016 seconds (0.004610538482666016 seconds from start)\n",
      "Wrote 0 of 278 utterances in 0.017353296279907227 seconds (0.017353296279907227 seconds from start)\n",
      "Wrote 0 of 104 utterances in 0.007387399673461914 seconds (0.007387399673461914 seconds from start)\n",
      "Wrote 0 of 222 utterances in 0.014041662216186523 seconds (0.014041662216186523 seconds from start)\n"
     ]
    }
   ],
   "source": [
    "# extract the full lists\n",
    "list_fnames = []\n",
    "for idx, intersection_utters in enumerate(intersections_utters):\n",
    "    # get file name for list and make dirs\n",
    "    name = clean_intersection_name(str(intersections_vals[idx]))\n",
    "    list_fnames.append(f\"/home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_{name}.txt\")\n",
    "    os.makedirs(os.path.dirname(list_fnames[-1]), exist_ok=True)\n",
    "    # write the list\n",
    "    make_pairs(intersection_utters, list_fnames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Australia.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_India.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Norway.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Ireland.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Germany.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_NewZealand.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Italy.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Mexico.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Sweden.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Spain.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Russia.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Switzerland.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Chile.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Philippines.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Croatia.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Denmark.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Netherlands.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Poland.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Portugal.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_China.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_France.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Guyana.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Singapore.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Brazil.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_SriLanka.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_SouthAfrica.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_SouthKorea.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_TrinidadandTobago.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Pakistan.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Austria.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Israel.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Iran.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_m_Sudan.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Australia.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_India.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Norway.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Ireland.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Germany.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_NewZealand.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Italy.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Mexico.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Sweden.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Spain.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Russia.txt\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Switzerland.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Chile.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Philippines.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Croatia.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Denmark.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Netherlands.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Poland.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Portugal.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_China.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_France.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Guyana.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Singapore.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Brazil.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_SriLanka.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_SouthAfrica.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_SouthKorea.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_TrinidadandTobago.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Pakistan.txt\n",
      "File not found... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Austria.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Israel.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Iran.txt\n",
      "File is empty... skipping\n",
      "Running /home/jupyter/voxceleb-fairness/data/lists/intersect/vox1_intersect_f_Sudan.txt\n",
      "File not found... skipping\n"
     ]
    }
   ],
   "source": [
    "# create the balanced lists with equal positive and negative test pairs\n",
    "def balance_pairs(unbalanced_pairs):\n",
    "    return pd.concat([unbalanced_pairs[unbalanced_pairs[0] == 1].reset_index(drop=True),\\\n",
    "               unbalanced_pairs[unbalanced_pairs[0] == 0].sample(sum(unbalanced_pairs[0] == 1)).\\\n",
    "               reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "\n",
    "for list_fname in list_fnames:\n",
    "    try:\n",
    "        print(f\"Running {list_fname}\")\n",
    "        # set new file name\n",
    "        new_fname = list_fname.replace('.txt', '_balanced.txt')\n",
    "        # read in old data and etract balanced list\n",
    "        pairs = pd.read_csv(list_fname, header=None, sep=' ')\n",
    "        balanced_pairs = balance_pairs(pairs)\n",
    "        # write the new list\n",
    "        balanced_pairs.to_csv(new_fname, index=False, header=None, sep=' ')\n",
    "    except ValueError:\n",
    "        print(f\">> File is empty... skipping\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\">> File not found... skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python [conda env:voxsrc-2020]",
   "language": "python",
   "name": "conda-env-voxsrc-2020-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
