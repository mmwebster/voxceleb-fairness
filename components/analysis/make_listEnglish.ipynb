{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, errno\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_csv('vox1_meta.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                    799\n",
       "UK                     215\n",
       "Canada                  54\n",
       "Australia               37\n",
       "India                   26\n",
       "Norway                  20\n",
       "Ireland                 18\n",
       "Germany                  9\n",
       "Italy                    8\n",
       "New Zealand              8\n",
       "Mexico                   7\n",
       "Sweden                   5\n",
       "Spain                    4\n",
       "Russia                   4\n",
       "Chile                    3\n",
       "Philippines              3\n",
       "Denmark                  3\n",
       "Netherlands              3\n",
       "Switzerland              3\n",
       "Croatia                  3\n",
       "China                    2\n",
       "Portugal                 2\n",
       "Poland                   2\n",
       "Israel                   1\n",
       "Iran                     1\n",
       "Trinidad and Tobago      1\n",
       "Singapore                1\n",
       "Sri Lanka                1\n",
       "Pakistan                 1\n",
       "South Africa             1\n",
       "France                   1\n",
       "Guyana                   1\n",
       "Sudan                    1\n",
       "Austria                  1\n",
       "Brazil                   1\n",
       "South Korea              1\n",
       "Name: Nationality, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta[\"Nationality\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VoxCeleb1 ID</th>\n",
       "      <th>VGGFace1 ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id10001</td>\n",
       "      <td>A.J._Buckley</td>\n",
       "      <td>m</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id10002</td>\n",
       "      <td>A.R._Rahman</td>\n",
       "      <td>m</td>\n",
       "      <td>India</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id10003</td>\n",
       "      <td>Aamir_Khan</td>\n",
       "      <td>m</td>\n",
       "      <td>India</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id10004</td>\n",
       "      <td>Aaron_Tveit</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id10005</td>\n",
       "      <td>Aaron_Yoo</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>id11247</td>\n",
       "      <td>Zachary_Levi</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>id11248</td>\n",
       "      <td>Zachary_Quinto</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>id11249</td>\n",
       "      <td>Zack_Snyder</td>\n",
       "      <td>m</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>id11250</td>\n",
       "      <td>Zoe_Saldana</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>id11251</td>\n",
       "      <td>Zulay_Henao</td>\n",
       "      <td>f</td>\n",
       "      <td>USA</td>\n",
       "      <td>dev</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1251 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     VoxCeleb1 ID     VGGFace1 ID Gender Nationality  Set\n",
       "0         id10001    A.J._Buckley      m     Ireland  dev\n",
       "1         id10002     A.R._Rahman      m       India  dev\n",
       "2         id10003      Aamir_Khan      m       India  dev\n",
       "3         id10004     Aaron_Tveit      m         USA  dev\n",
       "4         id10005       Aaron_Yoo      m         USA  dev\n",
       "...           ...             ...    ...         ...  ...\n",
       "1246      id11247    Zachary_Levi      m         USA  dev\n",
       "1247      id11248  Zachary_Quinto      m         USA  dev\n",
       "1248      id11249     Zack_Snyder      m         USA  dev\n",
       "1249      id11250     Zoe_Saldana      f         USA  dev\n",
       "1250      id11251     Zulay_Henao      f         USA  dev\n",
       "\n",
       "[1251 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GetIds(male=True):\n",
    "#     g = 'm' if male else 'f'\n",
    "#     return meta[meta[\"Gender\"] == g]['VoxCeleb1 ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male_ids = set(GetIds())\n",
    "# female_ids = set(GetIds(male=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_ids = set(meta[meta[\"Nationality\"] == 'USA']['VoxCeleb1 ID'])\n",
    "UK_ids = set(meta[meta[\"Nationality\"] == 'UK']['VoxCeleb1 ID'])\n",
    "Canada_ids = set(meta[meta[\"Nationality\"] == 'Canada']['VoxCeleb1 ID'])\n",
    "Australia_ids = set(meta[meta[\"Nationality\"] == 'Australia']['VoxCeleb1 ID'])\n",
    "India_ids = set(meta[meta[\"Nationality\"] == 'India']['VoxCeleb1 ID'])\n",
    "Norway_ids = set(meta[meta[\"Nationality\"] == 'Norway']['VoxCeleb1 ID'])\n",
    "Ireland_ids = set(meta[meta[\"Nationality\"] == 'Ireland']['VoxCeleb1 ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "utters = pd.read_csv('/home/jupyter/voxceleb-fairness/data/datasets/full/vox1_full_utterances.txt', header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         id10562\n",
       "1         id10562\n",
       "2         id10562\n",
       "3         id10562\n",
       "4         id10562\n",
       "           ...   \n",
       "153511    id10657\n",
       "153512    id10657\n",
       "153513    id10657\n",
       "153514    id10657\n",
       "153515    id10657\n",
       "Name: 0, Length: 153516, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_utters_temp = utters[[x in USA_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(USA_utters_temp) > 5000:\n",
    "    USA_utters = utters[[x in USA_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    USA_utters = USA_utters_temp\n",
    "##########################################    \n",
    "UK_utters_temp = utters[[x in UK_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(UK_utters_temp) > 5000:\n",
    "    UK_utters = utters[[x in UK_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    UK_utters = UK_utters_temp\n",
    "########################################### \n",
    "Canada_utters_temp = utters[[x in Canada_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(Canada_utters_temp) > 5000:\n",
    "    Canada_utters = utters[[x in Canada_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    Canada_utters = Canada_utters_temp\n",
    "###########################################\n",
    "Australia_utters_temp = utters[[x in Australia_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(Australia_utters_temp) > 5000:\n",
    "    Australia_utters = utters[[x in Australia_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    Australia_utters = Australia_utters_temp\n",
    "###########################################\n",
    "India_utters_temp = utters[[x in India_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(India_utters_temp) > 5000:\n",
    "    India_utters = utters[[x in India_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    India_utters = India_utters_temp\n",
    "###########################################\n",
    "Norway_utters_temp = utters[[x in Norway_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(Norway_utters_temp) > 5000:\n",
    "    Norway_utters = utters[[x in Norway_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    Norway_utters = Norway_utters_temp\n",
    "###########################################\n",
    "Ireland_utters_temp = utters[[x in Ireland_ids for x in utters[0]]].reset_index(drop=True)\n",
    "if len(Ireland_utters_temp) > 5000:\n",
    "    Ireland_utters = utters[[x in Ireland_ids for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "else:\n",
    "    Ireland_utters = Ireland_utters_temp\n",
    "###########################################\n",
    "#UK_utters = utters[[x in UK_ids for x in utters[0]]].reset_index(drop=True)##.sample(5000)\n",
    "#Canada_utters = utters[[x in Canada_ids for x in utters[0]]].reset_index(drop=True)##.sample(5000)\n",
    "#Australia_utters = utters[[x in Australia_ids for x in utters[0]]].reset_index(drop=True)##.sample(5000)\n",
    "#India_utters = utters[[x in India_ids for x in utters[0]]].reset_index(drop=True)##.sample(5000)\n",
    "#Norway_utters = utters[[x in Norway_ids for x in utters[0]]].reset_index(drop=True)##.sample(5000)\n",
    "#Ireland_utters = utters[[x in Ireland_ids for x in utters[0]]].reset_index(drop=True)##.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries = ['USA', \n",
    "#        'UK',\n",
    "#        'Canada', \n",
    "#        'Australia', \n",
    "#        'India', \n",
    "#        'Norway', \n",
    "#        'Ireland' ]\n",
    "# all_utters_names = []\n",
    "# for i, country_name in enumerate(countries):\n",
    "#     '{:s}'.format(country_name)+ '_utters_temp' = utters[[x in '{:s}'.format(country_name)+'_ids' for x in utters[0]]].reset_index(drop=True)\n",
    "#     if len('{:s}'.format(country_name)+'_utters_temp') > 5000:\n",
    "#         '{:s}'.format(country_name)+'_utters' = utters[[x in '{:s}'.format(country_name)+'_ids' for x in utters[0]]].sample(5000).reset_index(drop=True)\n",
    "#     else:\n",
    "#         '{:s}'.format(country_name)+'_utters' = '{:s}'.format(country_name)+'_ids'+'_utters_temp'\n",
    "#     all_utters_names.append(['{:s}'.format(country_name)+'_ids'+'_utters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USA_utters = all_utters_names[0]\n",
    "# UK_utters = all_utters_names[1]\n",
    "# Canada_utters = all_utters_names[2]\n",
    "# Australia_utters = all_utters_names[3]\n",
    "# India_utters = all_utters_names[4]\n",
    "# Norway_utters = all_utters_names[5]\n",
    "# Ireland_utters = all_utters_names[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# female_utters = utters[[x in female_ids for x in utters[0]]].sample(5000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silentremove(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "    except OSError as e: # this would be \"except OSError, e:\" before Python 2.6\n",
    "        if e.errno != errno.ENOENT: # errno.ENOENT = no such file or directory\n",
    "            raise # re-raise exception if a different error occurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(utters_df, filename):\n",
    "    silentremove(filename)\n",
    "    start = time.time()\n",
    "    current_iter_start = start\n",
    "    for i in range(len(utters_df)):\n",
    "        data = []\n",
    "        for j in range(i + 1, len(utters_df)):\n",
    "            num = 1 if utters_df[0][i] == utters_df[0][j] else 0\n",
    "            data.append([num, utters_df[1][i], utters_df[1][j]])\n",
    "        pd.DataFrame(data).to_csv(filename, mode='a', index=False, header=None, sep=' ')\n",
    "        if i % 1000 == 0:\n",
    "            current_iter_end = time.time()\n",
    "            print('Wrote {} of {} utterances in {} seconds ({} seconds from start)'.format(i, len(utters_df), current_iter_end - current_iter_start, current_iter_end - start))\n",
    "            current_iter_start = current_iter_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 0 of 5000 utterances in 0.12969708442687988 seconds (0.12969708442687988 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 110.4688720703125 seconds (110.59856915473938 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 85.56296610832214 seconds (196.16153526306152 seconds from start)\n",
      "Wrote 3000 of 5000 utterances in 61.37478566169739 seconds (257.5363209247589 seconds from start)\n",
      "Wrote 4000 of 5000 utterances in 37.4766058921814 seconds (295.0129268169403 seconds from start)\n",
      "Wrote 0 of 5000 utterances in 0.12234210968017578 seconds (0.12234210968017578 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 109.37769365310669 seconds (109.50003576278687 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 85.13477945327759 seconds (194.63481521606445 seconds from start)\n",
      "Wrote 3000 of 5000 utterances in 61.14364528656006 seconds (255.7784605026245 seconds from start)\n",
      "Wrote 4000 of 5000 utterances in 37.1662380695343 seconds (292.9446985721588 seconds from start)\n",
      "Wrote 0 of 5000 utterances in 0.11761927604675293 seconds (0.11761927604675293 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 108.3389527797699 seconds (108.45657205581665 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 84.87723445892334 seconds (193.33380651474 seconds from start)\n",
      "Wrote 3000 of 5000 utterances in 60.41188883781433 seconds (253.74569535255432 seconds from start)\n",
      "Wrote 4000 of 5000 utterances in 36.77790927886963 seconds (290.52360463142395 seconds from start)\n",
      "Wrote 0 of 4593 utterances in 0.10935807228088379 seconds (0.10935807228088379 seconds from start)\n",
      "Wrote 1000 of 4593 utterances in 97.74590301513672 seconds (97.8552610874176 seconds from start)\n",
      "Wrote 2000 of 4593 utterances in 75.28650140762329 seconds (173.1417624950409 seconds from start)\n",
      "Wrote 3000 of 4593 utterances in 50.483259439468384 seconds (223.62502193450928 seconds from start)\n",
      "Wrote 4000 of 4593 utterances in 27.157968759536743 seconds (250.78299069404602 seconds from start)\n",
      "Wrote 0 of 5000 utterances in 0.1196138858795166 seconds (0.1196138858795166 seconds from start)\n",
      "Wrote 1000 of 5000 utterances in 108.93906259536743 seconds (109.05867648124695 seconds from start)\n",
      "Wrote 2000 of 5000 utterances in 85.0522780418396 seconds (194.11095452308655 seconds from start)\n",
      "Wrote 3000 of 5000 utterances in 60.864381313323975 seconds (254.97533583641052 seconds from start)\n",
      "Wrote 4000 of 5000 utterances in 36.94728231430054 seconds (291.92261815071106 seconds from start)\n",
      "Wrote 0 of 2508 utterances in 0.06022953987121582 seconds (0.06022953987121582 seconds from start)\n",
      "Wrote 1000 of 2508 utterances in 48.632826805114746 seconds (48.69305634498596 seconds from start)\n",
      "Wrote 2000 of 2508 utterances in 25.29409146308899 seconds (73.98714780807495 seconds from start)\n",
      "Wrote 0 of 2567 utterances in 0.06771230697631836 seconds (0.06771230697631836 seconds from start)\n",
      "Wrote 1000 of 2567 utterances in 50.28590226173401 seconds (50.35361456871033 seconds from start)\n",
      "Wrote 2000 of 2567 utterances in 26.675469160079956 seconds (77.02908372879028 seconds from start)\n"
     ]
    }
   ],
   "source": [
    "make_pairs(USA_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_USA_full.txt')\n",
    "make_pairs(UK_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_UK_full.txt')\n",
    "make_pairs(Canada_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_Canada_full.txt')\n",
    "make_pairs(Australia_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_Australia_full.txt')\n",
    "make_pairs(India_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_India_full.txt')\n",
    "make_pairs(Norway_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_Norway_full.txt')\n",
    "make_pairs(Ireland_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_Ireland_full.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "USA_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_USA_full.txt', header=None, sep=' ')\n",
    "UK_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_UK_full.txt', header=None, sep=' ')\n",
    "Canada_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Canada_full.txt', header=None, sep=' ')\n",
    "Australia_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Australia_full.txt', header=None, sep=' ')\n",
    "India_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_India_full.txt', header=None, sep=' ')\n",
    "Norway_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Norway_full.txt', header=None, sep=' ')\n",
    "Ireland_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Ireland_full.txt', header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_USA_pairs = pd.concat([USA_pairs[USA_pairs[0] == 1].reset_index(drop=True), USA_pairs[USA_pairs[0] == 0].sample(sum(USA_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "balanced_UK_pairs = pd.concat([UK_pairs[UK_pairs[0] == 1].reset_index(drop=True), UK_pairs[UK_pairs[0] == 0].sample(sum(UK_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "balanced_Canada_pairs = pd.concat([Canada_pairs[Canada_pairs[0] == 1].reset_index(drop=True), Canada_pairs[Canada_pairs[0] == 0].sample(sum(Canada_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "balanced_Australia_pairs = pd.concat([Australia_pairs[Australia_pairs[0] == 1].reset_index(drop=True), Australia_pairs[Australia_pairs[0] == 0].sample(sum(Australia_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "balanced_India_pairs = pd.concat([India_pairs[India_pairs[0] == 1].reset_index(drop=True), India_pairs[India_pairs[0] == 0].sample(sum(India_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "balanced_Norway_pairs = pd.concat([Norway_pairs[Norway_pairs[0] == 1].reset_index(drop=True), Norway_pairs[Norway_pairs[0] == 0].sample(sum(Norway_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n",
    "balanced_Ireland_pairs = pd.concat([Ireland_pairs[Ireland_pairs[0] == 1].reset_index(drop=True), Ireland_pairs[Ireland_pairs[0] == 0].sample(sum(Ireland_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced_USA_pairs\n",
    "# balanced_UK_pairs\n",
    "# Canada_pairs\n",
    "# Australia_pairs\n",
    "# balanced_India_pairs\n",
    "# balanced_Norway_pairs\n",
    "# balanced_Ireland_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_pairs(female_utters, '/home/jupyter/voxceleb-fairness/data/lists/vox1_female_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# female_pairs = pd.read_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_female_all.txt', header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_female_pairs = pd.concat([female_pairs[female_pairs[0] == 1].reset_index(drop=True), female_pairs[female_pairs[0] == 0].sample(sum(female_pairs[0] == 1)).reset_index(drop=True)]).sort_index().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_female_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_USA_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_USA_full_balanced.txt', index=False, header=None, sep=' ')\n",
    "balanced_UK_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_UK_full_balanced.txt', index=False, header=None, sep=' ')\n",
    "balanced_Canada_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Canada_full_balanced.txt', index=False, header=None, sep=' ')\n",
    "balanced_Australia_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Australia_full_balanced.txt', index=False, header=None, sep=' ')\n",
    "balanced_India_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_India_full_balanced.txt', index=False, header=None, sep=' ')\n",
    "balanced_Norway_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Normay_full_balanced.txt', index=False, header=None, sep=' ')\n",
    "balanced_Ireland_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_Ireland_full_balanced.txt', index=False, header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced_female_pairs.to_csv('/home/jupyter/voxceleb-fairness/data/lists/vox1_female_all_balanced.txt', index=False, header=None, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-6.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-6:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
